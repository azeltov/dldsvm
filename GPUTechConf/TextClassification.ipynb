{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Keras\n",
    "Author: [Valentin Malykh](http://val.maly.hk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we are going to introduce Natural Language Processing (NLP) through the task of text classification. Here, we train a neural network to identify *types* of text samples: positive samples from negative samples, samples from different writers, etc.\n",
    "\n",
    "Like many challenges in deep learning, we're working to train a neural network to map between provided inputs and desired outputs. Let's start by understanding how to prepare text for input to a neural network. \n",
    "\n",
    "## Input data\n",
    "\n",
    "First, let us define some terms used in Natural Language Processing (NLP) to describe language as input data:\n",
    "\n",
    "- *token* - A unit of text, it could be a word (and almost always is), but also it could be a group of words like \"New York\", a sub-word like \"mega\" in \"megabyte\", or a letter like \"m\". Each token can be represented as a distinct number in a process called *tokenizing*.  \n",
    "- *document* - A sequence of *tokens,* this could be whole book or a tweet. In this case, we're going to classify each *document* as having a positive or a negative sentiment.  \n",
    "- *corpus* - A set of *documents.* You can think of this as your \"dataset\". If you want to learn about language in general, you might use a *corpus* like Wikipedia. In this case, we'll use a *corpus* containing movie reviews since each *document* is paired with a rating indicating *sentiment.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Steps\n",
    "\n",
    "In this lab, we are going to perform *text classification* for *sentiment analysis*. We will classify each document into one of 2 different sentiments: \"positive\" or \"negative\". \n",
    "\n",
    "Our training *corpus* will be the Sentiment Tree Bank from [Stanford's NLP group](https://nlp.stanford.edu/sentiment/). The dataset's creators at Stanford describe the benefit of using Deep Learning for language processing vs. using traditional methods below:\n",
    "\n",
    "```\"Most sentiment prediction systems work just by looking at words in isolation, giving positive points for positive words and negative points for negative words and then summing up these points. That way, the order of words is ignored, and important information is lost. In contrast, our new deep learning model builds a representation of whole sentences based on the sentence structure. It computes the sentiment based on how words compose the meaning of longer phrases. This way, the model is not as easily fooled as previous models. For example, our model learned that funny and witty are positive, but the following sentence is still negative overall:```\n",
    "\n",
    "*This movie was actually neither that funny, nor super witty.*\"\n",
    "\n",
    "The idea is to understand the overall context of each word used along with its meaning to figure out the emotional tone of the document. Look at the [dataset](https://nlp.stanford.edu/sentiment/treebank.html) before loading libraries, downloading the dataset, and unzipping it into our workspace below.\n",
    "\n",
    "This Jupyter Notebook runs python code in code blocks like the one below. Click inside the cell and simultaneously press **Shift + Enter** to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  stanfordSentimentTreebank.zip\n",
      "replace stanfordSentimentTreebank/datasetSentences.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas\n",
    "! if [ ! -f stanfordSentimentTreebank.zip ]; then wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip; fi\n",
    "! unzip stanfordSentimentTreebank.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view and manage the data, we'll use a tool called **Pandas**. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools. Our corpus is currently stored in a CSV. We'll use Pandas here to read and view CSV data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index                                           sentence\n",
       "0               1  The Rock is destined to be the 21st Century 's...\n",
       "1               2  The gorgeously elaborate continuation of `` Th...\n",
       "2               3                     Effective but too-tepid biopic\n",
       "3               4  If you sometimes like to go to the movies to h...\n",
       "4               5  Emerges as something rare , an issue movie tha..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pandas.read_csv(\"stanfordSentimentTreebank/datasetSentences.txt\", sep=\"\\t\")\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see (if you scroll to the bottom) that our corpus contained 11855 sentences each with a unique identifier.\n",
    "\n",
    "Next, we need each sentence's corresponding label to identify if the sentence contained a positive or negative sentiment. \n",
    "\n",
    "When we do a *read* with pandas, it creates a base object called DataFrame. DataFrame is represented as a *numpy array*, an ideal format for feeding to a neural network.\n",
    "\n",
    "Below, we will use another property of DataFrame, column access. ```sentences[\"sentence\"]``` will return only one specific column of this particular DataFrame, the one labeled \"sentence\". ```tolist()``` returns a python list instead of a Series (another base class in Pandas). Execute the following cells to prepare the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_labels(sentences):\n",
    "    dictionary = dict()\n",
    "    with open(\"stanfordSentimentTreebank/dictionary.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            splitted = line.split(\"|\")\n",
    "            dictionary[splitted[0].lower()] = int(splitted[1])\n",
    "\n",
    "    labels = [0.5] * (max(dictionary.values()) + 1)\n",
    "    with open(\"stanfordSentimentTreebank/sentiment_labels.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            splitted = line.split(\"|\")\n",
    "            labels[int(splitted[0])] = float(splitted[1])\n",
    "\n",
    "    sent_labels = [0.5] * len(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        full_sent = sentences[i].replace(\"-lrb-\", \"(\").replace(\"-rrb-\", \")\").replace(\"\\\\\\\\\", \"\")\n",
    "        try:\n",
    "            sent_labels[i] = labels[dictionary[full_sent.lower()]]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return sent_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create labels and check how many sentences there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11855"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = sent_labels(sentences=sentences[\"sentence\"].tolist())\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a large enough dataset for us to start working with. \n",
    "\n",
    "Lastly, like any deep learning workflow, we should separate our dataset into a training set (that our network will learn from), a validation set (that our network will *NOT* learn from, so that we can see if our model is effective with *new* data) and a test set (that our network will *NOT* learn from, used to visualize what *types* of sentences confuse our model.)\n",
    "\n",
    "The creators at Stanford have helped us do this by including a \"split\" csv. Let's load that with Pandas now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>splitset_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence_index  splitset_label\n",
       "0               1               1\n",
       "1               2               1\n",
       "2               3               2\n",
       "3               4               2\n",
       "4               5               2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split = pandas.read_csv(\"stanfordSentimentTreebank/datasetSplit.txt\")\n",
    "split.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here the same unique identifiers as with the sentences above along with a split label: 1, 2, or 3. The sentences and labels that correspond to 1 belong to our training dataset, 2 belong to our validation set, and 3 belong to our test set.\n",
    "\n",
    "Lastly, we can concatenate our sentences, labels, and split indicators to create our comprehensive dataset. Note that ```concat``` will concatenate DataFrames (and Series) even if they are of different lengths. This flexibility is another reason we are utilizing Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>0</th>\n",
       "      <th>sentence_index</th>\n",
       "      <th>splitset_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>0.694440</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>0.833330</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Effective but too-tepid biopic</td>\n",
       "      <td>0.513890</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>If you sometimes like to go to the movies to h...</td>\n",
       "      <td>0.736110</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Emerges as something rare , an issue movie tha...</td>\n",
       "      <td>0.861110</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>The film provides some great insight into the ...</td>\n",
       "      <td>0.597220</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>Offers that rare combination of entertainment ...</td>\n",
       "      <td>0.833330</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>Perhaps no picture ever made has more literall...</td>\n",
       "      <td>0.694440</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>Steers turns in a snappy screenplay that curls...</td>\n",
       "      <td>0.777780</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>But he somehow pulls it off .</td>\n",
       "      <td>0.736110</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>Take Care of My Cat offers a refreshingly diff...</td>\n",
       "      <td>0.763890</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>This is a film well worth seeing , talking and...</td>\n",
       "      <td>0.902780</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>What really surprises about Wisegirls is its l...</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>-LRB- Wendigo is -RRB- why we go to the cinema...</td>\n",
       "      <td>0.652780</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>One of the greatest family-oriented , fantasy-...</td>\n",
       "      <td>0.930560</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>Ultimately , it ponders the reasons we need st...</td>\n",
       "      <td>0.458330</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>An utterly compelling ` who wrote it ' in whic...</td>\n",
       "      <td>0.777780</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>Illuminating if overly talky documentary .</td>\n",
       "      <td>0.472220</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>A masterpiece four years in the making .</td>\n",
       "      <td>0.972220</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>The movie 's ripe , enrapturing beauty will te...</td>\n",
       "      <td>0.763890</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>Offers a breath of the fresh air of true sophi...</td>\n",
       "      <td>0.888890</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>A thoughtful , provocative , insistently human...</td>\n",
       "      <td>0.819440</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>With a cast that includes some of the top acto...</td>\n",
       "      <td>0.861110</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>A disturbing and frighteningly evocative assem...</td>\n",
       "      <td>0.583330</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>Not for everyone , but for those with whom it ...</td>\n",
       "      <td>0.736110</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>Scores a few points for doing what it does wit...</td>\n",
       "      <td>0.722220</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>Occasionally melodramatic , it 's also extreme...</td>\n",
       "      <td>0.777780</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>An idealistic love story that brings out the l...</td>\n",
       "      <td>0.680560</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>At about 95 minutes , Treasure Planet maintain...</td>\n",
       "      <td>0.736110</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>However , it lacks grandeur and that epic qual...</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11825</th>\n",
       "      <td>11826</td>\n",
       "      <td>A reality-snubbing hodgepodge .</td>\n",
       "      <td>0.222220</td>\n",
       "      <td>11826</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11826</th>\n",
       "      <td>11827</td>\n",
       "      <td>Mildly amusing .</td>\n",
       "      <td>0.597220</td>\n",
       "      <td>11827</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>11828</td>\n",
       "      <td>Fairly run-of-the-mill .</td>\n",
       "      <td>0.319440</td>\n",
       "      <td>11828</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11828</th>\n",
       "      <td>11829</td>\n",
       "      <td>Mildly entertaining .</td>\n",
       "      <td>0.416670</td>\n",
       "      <td>11829</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11829</th>\n",
       "      <td>11830</td>\n",
       "      <td>Terrible .</td>\n",
       "      <td>0.111110</td>\n",
       "      <td>11830</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11830</th>\n",
       "      <td>11831</td>\n",
       "      <td>Degenerates into hogwash .</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>11831</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11831</th>\n",
       "      <td>11832</td>\n",
       "      <td>Meandering and confusing .</td>\n",
       "      <td>0.236110</td>\n",
       "      <td>11832</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11832</th>\n",
       "      <td>11833</td>\n",
       "      <td>Crummy .</td>\n",
       "      <td>0.138890</td>\n",
       "      <td>11833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11833</th>\n",
       "      <td>11834</td>\n",
       "      <td>An opportunity missed .</td>\n",
       "      <td>0.222220</td>\n",
       "      <td>11834</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11834</th>\n",
       "      <td>11835</td>\n",
       "      <td>Wishy-washy .</td>\n",
       "      <td>0.222220</td>\n",
       "      <td>11835</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11835</th>\n",
       "      <td>11836</td>\n",
       "      <td>Inconsequential road-and-buddy pic .</td>\n",
       "      <td>0.388890</td>\n",
       "      <td>11836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11836</th>\n",
       "      <td>11837</td>\n",
       "      <td>Insufferably naive .</td>\n",
       "      <td>0.277780</td>\n",
       "      <td>11837</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11837</th>\n",
       "      <td>11838</td>\n",
       "      <td>Ill-considered , unholy hokum .</td>\n",
       "      <td>0.263890</td>\n",
       "      <td>11838</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11838</th>\n",
       "      <td>11839</td>\n",
       "      <td>Amazingly lame .</td>\n",
       "      <td>0.166670</td>\n",
       "      <td>11839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11839</th>\n",
       "      <td>11840</td>\n",
       "      <td>-LRB- A -RRB- slummer .</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11840</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11840</th>\n",
       "      <td>11841</td>\n",
       "      <td>-LRB- A -RRB- poorly executed comedy .</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>11842</td>\n",
       "      <td>... really horrible drek .</td>\n",
       "      <td>0.111110</td>\n",
       "      <td>11842</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11842</th>\n",
       "      <td>11843</td>\n",
       "      <td>An intriguing near-miss .</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>11843</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11843</th>\n",
       "      <td>11844</td>\n",
       "      <td>Flat , misguided comedy .</td>\n",
       "      <td>0.194440</td>\n",
       "      <td>11844</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11844</th>\n",
       "      <td>11845</td>\n",
       "      <td>Predictably melodramatic .</td>\n",
       "      <td>0.291670</td>\n",
       "      <td>11845</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11845</th>\n",
       "      <td>11846</td>\n",
       "      <td>Rashomon-for-dipsticks tale .</td>\n",
       "      <td>0.180560</td>\n",
       "      <td>11846</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11846</th>\n",
       "      <td>11847</td>\n",
       "      <td>Bearable .</td>\n",
       "      <td>0.430560</td>\n",
       "      <td>11847</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11847</th>\n",
       "      <td>11848</td>\n",
       "      <td>Barely .</td>\n",
       "      <td>0.458330</td>\n",
       "      <td>11848</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11848</th>\n",
       "      <td>11849</td>\n",
       "      <td>Staggeringly dreadful romance .</td>\n",
       "      <td>0.152780</td>\n",
       "      <td>11849</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11849</th>\n",
       "      <td>11850</td>\n",
       "      <td>Well-made but mush-hearted .</td>\n",
       "      <td>0.638890</td>\n",
       "      <td>11850</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11850</th>\n",
       "      <td>11851</td>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>0.111110</td>\n",
       "      <td>11851</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11851</th>\n",
       "      <td>11852</td>\n",
       "      <td>No surprises .</td>\n",
       "      <td>0.222220</td>\n",
       "      <td>11852</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11852</th>\n",
       "      <td>11853</td>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>11853</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11853</th>\n",
       "      <td>11854</td>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>0.138890</td>\n",
       "      <td>11854</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11854</th>\n",
       "      <td>11855</td>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>0.347220</td>\n",
       "      <td>11855</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11855 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_index                                           sentence  \\\n",
       "0                   1  The Rock is destined to be the 21st Century 's...   \n",
       "1                   2  The gorgeously elaborate continuation of `` Th...   \n",
       "2                   3                     Effective but too-tepid biopic   \n",
       "3                   4  If you sometimes like to go to the movies to h...   \n",
       "4                   5  Emerges as something rare , an issue movie tha...   \n",
       "5                   6  The film provides some great insight into the ...   \n",
       "6                   7  Offers that rare combination of entertainment ...   \n",
       "7                   8  Perhaps no picture ever made has more literall...   \n",
       "8                   9  Steers turns in a snappy screenplay that curls...   \n",
       "9                  10                      But he somehow pulls it off .   \n",
       "10                 11  Take Care of My Cat offers a refreshingly diff...   \n",
       "11                 12  This is a film well worth seeing , talking and...   \n",
       "12                 13  What really surprises about Wisegirls is its l...   \n",
       "13                 14  -LRB- Wendigo is -RRB- why we go to the cinema...   \n",
       "14                 15  One of the greatest family-oriented , fantasy-...   \n",
       "15                 16  Ultimately , it ponders the reasons we need st...   \n",
       "16                 17  An utterly compelling ` who wrote it ' in whic...   \n",
       "17                 18         Illuminating if overly talky documentary .   \n",
       "18                 19           A masterpiece four years in the making .   \n",
       "19                 20  The movie 's ripe , enrapturing beauty will te...   \n",
       "20                 21  Offers a breath of the fresh air of true sophi...   \n",
       "21                 22  A thoughtful , provocative , insistently human...   \n",
       "22                 23  With a cast that includes some of the top acto...   \n",
       "23                 24  A disturbing and frighteningly evocative assem...   \n",
       "24                 25  Not for everyone , but for those with whom it ...   \n",
       "25                 26  Scores a few points for doing what it does wit...   \n",
       "26                 27  Occasionally melodramatic , it 's also extreme...   \n",
       "27                 28  An idealistic love story that brings out the l...   \n",
       "28                 29  At about 95 minutes , Treasure Planet maintain...   \n",
       "29                 30  However , it lacks grandeur and that epic qual...   \n",
       "...               ...                                                ...   \n",
       "11825           11826                    A reality-snubbing hodgepodge .   \n",
       "11826           11827                                   Mildly amusing .   \n",
       "11827           11828                           Fairly run-of-the-mill .   \n",
       "11828           11829                              Mildly entertaining .   \n",
       "11829           11830                                         Terrible .   \n",
       "11830           11831                         Degenerates into hogwash .   \n",
       "11831           11832                         Meandering and confusing .   \n",
       "11832           11833                                           Crummy .   \n",
       "11833           11834                            An opportunity missed .   \n",
       "11834           11835                                      Wishy-washy .   \n",
       "11835           11836               Inconsequential road-and-buddy pic .   \n",
       "11836           11837                               Insufferably naive .   \n",
       "11837           11838                    Ill-considered , unholy hokum .   \n",
       "11838           11839                                   Amazingly lame .   \n",
       "11839           11840                            -LRB- A -RRB- slummer .   \n",
       "11840           11841             -LRB- A -RRB- poorly executed comedy .   \n",
       "11841           11842                         ... really horrible drek .   \n",
       "11842           11843                          An intriguing near-miss .   \n",
       "11843           11844                          Flat , misguided comedy .   \n",
       "11844           11845                         Predictably melodramatic .   \n",
       "11845           11846                      Rashomon-for-dipsticks tale .   \n",
       "11846           11847                                         Bearable .   \n",
       "11847           11848                                           Barely .   \n",
       "11848           11849                    Staggeringly dreadful romance .   \n",
       "11849           11850                       Well-made but mush-hearted .   \n",
       "11850           11851                                    A real snooze .   \n",
       "11851           11852                                     No surprises .   \n",
       "11852           11853  We 've seen the hippie-turned-yuppie plot befo...   \n",
       "11853           11854  Her fans walked out muttering words like `` ho...   \n",
       "11854           11855                                In this case zero .   \n",
       "\n",
       "              0  sentence_index  splitset_label  \n",
       "0      0.694440               1               1  \n",
       "1      0.833330               2               1  \n",
       "2      0.513890               3               2  \n",
       "3      0.736110               4               2  \n",
       "4      0.861110               5               2  \n",
       "5      0.597220               6               2  \n",
       "6      0.833330               7               2  \n",
       "7      0.694440               8               2  \n",
       "8      0.777780               9               2  \n",
       "9      0.736110              10               2  \n",
       "10     0.763890              11               2  \n",
       "11     0.902780              12               2  \n",
       "12     0.625000              13               2  \n",
       "13     0.652780              14               2  \n",
       "14     0.930560              15               2  \n",
       "15     0.458330              16               2  \n",
       "16     0.777780              17               2  \n",
       "17     0.472220              18               2  \n",
       "18     0.972220              19               2  \n",
       "19     0.763890              20               2  \n",
       "20     0.888890              21               2  \n",
       "21     0.819440              22               2  \n",
       "22     0.861110              23               2  \n",
       "23     0.583330              24               2  \n",
       "24     0.736110              25               2  \n",
       "25     0.722220              26               2  \n",
       "26     0.777780              27               2  \n",
       "27     0.680560              28               2  \n",
       "28     0.736110              29               2  \n",
       "29     0.250000              30               2  \n",
       "...         ...             ...             ...  \n",
       "11825  0.222220           11826               1  \n",
       "11826  0.597220           11827               1  \n",
       "11827  0.319440           11828               1  \n",
       "11828  0.416670           11829               1  \n",
       "11829  0.111110           11830               1  \n",
       "11830  0.055556           11831               1  \n",
       "11831  0.236110           11832               1  \n",
       "11832  0.138890           11833               1  \n",
       "11833  0.222220           11834               1  \n",
       "11834  0.222220           11835               1  \n",
       "11835  0.388890           11836               1  \n",
       "11836  0.277780           11837               1  \n",
       "11837  0.263890           11838               1  \n",
       "11838  0.166670           11839               1  \n",
       "11839  0.500000           11840               1  \n",
       "11840  0.500000           11841               1  \n",
       "11841  0.111110           11842               1  \n",
       "11842  0.500000           11843               1  \n",
       "11843  0.194440           11844               1  \n",
       "11844  0.291670           11845               1  \n",
       "11845  0.180560           11846               1  \n",
       "11846  0.430560           11847               1  \n",
       "11847  0.458330           11848               1  \n",
       "11848  0.152780           11849               1  \n",
       "11849  0.638890           11850               1  \n",
       "11850  0.111110           11851               1  \n",
       "11851  0.222220           11852               1  \n",
       "11852  0.750000           11853               1  \n",
       "11853  0.138890           11854               1  \n",
       "11854  0.347220           11855               1  \n",
       "\n",
       "[11855 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pandas.concat([sentences, pandas.DataFrame(labels), split], axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the following cell to see how you can use Pandas to filter. By changing \"splitset_label\" from 1 to 2 to 3, you will see the training set, validation set, and test set respectively. \n",
    "\n",
    "First select some columns by their names - ```dataset[[\"sentence\",0,\"splitset_label\"]]```, and next filter the produced DataFrame by value of one of its columns ```d[d[\"splitset_label\"] == 1]```.\n",
    "\n",
    "Also, if you call a DataFrame in Jupyter, it is an equivalent of ```head()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>0</th>\n",
       "      <th>splitset_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>0.694440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>0.833330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>0.722220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>Whether or not you 're enlightened by any of D...</td>\n",
       "      <td>0.833330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Just the labour involved in creating the layer...</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>Part of the charm of Satin Rouge is that it av...</td>\n",
       "      <td>0.722220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>a screenplay more ingeniously constructed than...</td>\n",
       "      <td>0.833330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>`` Extreme Ops '' exceeds expectations .</td>\n",
       "      <td>0.736110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>Good fun , good action , good acting , good di...</td>\n",
       "      <td>0.902780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>You Should Pay Nine Bucks for This : Because y...</td>\n",
       "      <td>0.444440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>Dramas like this make it human .</td>\n",
       "      <td>0.805560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>A thunderous ride at first , quiet cadences of...</td>\n",
       "      <td>0.444440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>Still , this flick is fun , and host to some t...</td>\n",
       "      <td>0.819440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Australian actor\\/director John Polson and awa...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>You walk out of The Good Girl with mixed emoti...</td>\n",
       "      <td>0.611110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Post 9\\/11 the philosophical message of `` Per...</td>\n",
       "      <td>0.444440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>Absorbing character study by AndrÃ© Turpin .</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>If you love reading and\\/or poetry , then by a...</td>\n",
       "      <td>0.777780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>You 'll probably love it .</td>\n",
       "      <td>0.819440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427</th>\n",
       "      <td>`` Frailty '' has been written so well , that ...</td>\n",
       "      <td>0.638890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>428</th>\n",
       "      <td>near the end takes on a whole other meaning .</td>\n",
       "      <td>0.555560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Grenier is terrific , bringing an unforced , r...</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>466</th>\n",
       "      <td>The Sundance Film Festival has become so buzz-...</td>\n",
       "      <td>0.555560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>467</th>\n",
       "      <td>` Tadpole ' was one of the films so declared t...</td>\n",
       "      <td>0.513890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>472</th>\n",
       "      <td>The actors are fantastic .</td>\n",
       "      <td>0.944440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>They are what makes it worth the trip to the t...</td>\n",
       "      <td>0.722220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>-LRB- Taymor -RRB- utilizes the idea of making...</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>This is n't a new idea .</td>\n",
       "      <td>0.333330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11825</th>\n",
       "      <td>A reality-snubbing hodgepodge .</td>\n",
       "      <td>0.222220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11826</th>\n",
       "      <td>Mildly amusing .</td>\n",
       "      <td>0.597220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11827</th>\n",
       "      <td>Fairly run-of-the-mill .</td>\n",
       "      <td>0.319440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11828</th>\n",
       "      <td>Mildly entertaining .</td>\n",
       "      <td>0.416670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11829</th>\n",
       "      <td>Terrible .</td>\n",
       "      <td>0.111110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11830</th>\n",
       "      <td>Degenerates into hogwash .</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11831</th>\n",
       "      <td>Meandering and confusing .</td>\n",
       "      <td>0.236110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11832</th>\n",
       "      <td>Crummy .</td>\n",
       "      <td>0.138890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11833</th>\n",
       "      <td>An opportunity missed .</td>\n",
       "      <td>0.222220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11834</th>\n",
       "      <td>Wishy-washy .</td>\n",
       "      <td>0.222220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11835</th>\n",
       "      <td>Inconsequential road-and-buddy pic .</td>\n",
       "      <td>0.388890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11836</th>\n",
       "      <td>Insufferably naive .</td>\n",
       "      <td>0.277780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11837</th>\n",
       "      <td>Ill-considered , unholy hokum .</td>\n",
       "      <td>0.263890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11838</th>\n",
       "      <td>Amazingly lame .</td>\n",
       "      <td>0.166670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11839</th>\n",
       "      <td>-LRB- A -RRB- slummer .</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11840</th>\n",
       "      <td>-LRB- A -RRB- poorly executed comedy .</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11841</th>\n",
       "      <td>... really horrible drek .</td>\n",
       "      <td>0.111110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11842</th>\n",
       "      <td>An intriguing near-miss .</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11843</th>\n",
       "      <td>Flat , misguided comedy .</td>\n",
       "      <td>0.194440</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11844</th>\n",
       "      <td>Predictably melodramatic .</td>\n",
       "      <td>0.291670</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11845</th>\n",
       "      <td>Rashomon-for-dipsticks tale .</td>\n",
       "      <td>0.180560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11846</th>\n",
       "      <td>Bearable .</td>\n",
       "      <td>0.430560</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11847</th>\n",
       "      <td>Barely .</td>\n",
       "      <td>0.458330</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11848</th>\n",
       "      <td>Staggeringly dreadful romance .</td>\n",
       "      <td>0.152780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11849</th>\n",
       "      <td>Well-made but mush-hearted .</td>\n",
       "      <td>0.638890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11850</th>\n",
       "      <td>A real snooze .</td>\n",
       "      <td>0.111110</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11851</th>\n",
       "      <td>No surprises .</td>\n",
       "      <td>0.222220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11852</th>\n",
       "      <td>We 've seen the hippie-turned-yuppie plot befo...</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11853</th>\n",
       "      <td>Her fans walked out muttering words like `` ho...</td>\n",
       "      <td>0.138890</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11854</th>\n",
       "      <td>In this case zero .</td>\n",
       "      <td>0.347220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8544 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence         0  \\\n",
       "0      The Rock is destined to be the 21st Century 's...  0.694440   \n",
       "1      The gorgeously elaborate continuation of `` Th...  0.833330   \n",
       "60     Singer\\/composer Bryan Adams contributes a sle...  0.625000   \n",
       "61     You 'd think by now America would have had eno...  0.500000   \n",
       "62                  Yet the act is still charming here .  0.722220   \n",
       "63     Whether or not you 're enlightened by any of D...  0.833330   \n",
       "67     Just the labour involved in creating the layer...  0.875000   \n",
       "71     Part of the charm of Satin Rouge is that it av...  0.722220   \n",
       "81     a screenplay more ingeniously constructed than...  0.833330   \n",
       "130             `` Extreme Ops '' exceeds expectations .  0.736110   \n",
       "131    Good fun , good action , good acting , good di...  0.902780   \n",
       "132    You Should Pay Nine Bucks for This : Because y...  0.444440   \n",
       "133                     Dramas like this make it human .  0.805560   \n",
       "134    A thunderous ride at first , quiet cadences of...  0.444440   \n",
       "135    Still , this flick is fun , and host to some t...  0.819440   \n",
       "213    Australian actor\\/director John Polson and awa...  0.750000   \n",
       "227    You walk out of The Good Girl with mixed emoti...  0.611110   \n",
       "339    Post 9\\/11 the philosophical message of `` Per...  0.444440   \n",
       "382         Absorbing character study by AndrÃ© Turpin .  0.500000   \n",
       "386    If you love reading and\\/or poetry , then by a...  0.777780   \n",
       "387                           You 'll probably love it .  0.819440   \n",
       "427    `` Frailty '' has been written so well , that ...  0.638890   \n",
       "428        near the end takes on a whole other meaning .  0.555560   \n",
       "445    Grenier is terrific , bringing an unforced , r...  0.875000   \n",
       "466    The Sundance Film Festival has become so buzz-...  0.555560   \n",
       "467    ` Tadpole ' was one of the films so declared t...  0.513890   \n",
       "472                           The actors are fantastic .  0.944440   \n",
       "473    They are what makes it worth the trip to the t...  0.722220   \n",
       "485    -LRB- Taymor -RRB- utilizes the idea of making...  0.500000   \n",
       "486                             This is n't a new idea .  0.333330   \n",
       "...                                                  ...       ...   \n",
       "11825                    A reality-snubbing hodgepodge .  0.222220   \n",
       "11826                                   Mildly amusing .  0.597220   \n",
       "11827                           Fairly run-of-the-mill .  0.319440   \n",
       "11828                              Mildly entertaining .  0.416670   \n",
       "11829                                         Terrible .  0.111110   \n",
       "11830                         Degenerates into hogwash .  0.055556   \n",
       "11831                         Meandering and confusing .  0.236110   \n",
       "11832                                           Crummy .  0.138890   \n",
       "11833                            An opportunity missed .  0.222220   \n",
       "11834                                      Wishy-washy .  0.222220   \n",
       "11835               Inconsequential road-and-buddy pic .  0.388890   \n",
       "11836                               Insufferably naive .  0.277780   \n",
       "11837                    Ill-considered , unholy hokum .  0.263890   \n",
       "11838                                   Amazingly lame .  0.166670   \n",
       "11839                            -LRB- A -RRB- slummer .  0.500000   \n",
       "11840             -LRB- A -RRB- poorly executed comedy .  0.500000   \n",
       "11841                         ... really horrible drek .  0.111110   \n",
       "11842                          An intriguing near-miss .  0.500000   \n",
       "11843                          Flat , misguided comedy .  0.194440   \n",
       "11844                         Predictably melodramatic .  0.291670   \n",
       "11845                      Rashomon-for-dipsticks tale .  0.180560   \n",
       "11846                                         Bearable .  0.430560   \n",
       "11847                                           Barely .  0.458330   \n",
       "11848                    Staggeringly dreadful romance .  0.152780   \n",
       "11849                       Well-made but mush-hearted .  0.638890   \n",
       "11850                                    A real snooze .  0.111110   \n",
       "11851                                     No surprises .  0.222220   \n",
       "11852  We 've seen the hippie-turned-yuppie plot befo...  0.750000   \n",
       "11853  Her fans walked out muttering words like `` ho...  0.138890   \n",
       "11854                                In this case zero .  0.347220   \n",
       "\n",
       "       splitset_label  \n",
       "0                   1  \n",
       "1                   1  \n",
       "60                  1  \n",
       "61                  1  \n",
       "62                  1  \n",
       "63                  1  \n",
       "67                  1  \n",
       "71                  1  \n",
       "81                  1  \n",
       "130                 1  \n",
       "131                 1  \n",
       "132                 1  \n",
       "133                 1  \n",
       "134                 1  \n",
       "135                 1  \n",
       "213                 1  \n",
       "227                 1  \n",
       "339                 1  \n",
       "382                 1  \n",
       "386                 1  \n",
       "387                 1  \n",
       "427                 1  \n",
       "428                 1  \n",
       "445                 1  \n",
       "466                 1  \n",
       "467                 1  \n",
       "472                 1  \n",
       "473                 1  \n",
       "485                 1  \n",
       "486                 1  \n",
       "...               ...  \n",
       "11825               1  \n",
       "11826               1  \n",
       "11827               1  \n",
       "11828               1  \n",
       "11829               1  \n",
       "11830               1  \n",
       "11831               1  \n",
       "11832               1  \n",
       "11833               1  \n",
       "11834               1  \n",
       "11835               1  \n",
       "11836               1  \n",
       "11837               1  \n",
       "11838               1  \n",
       "11839               1  \n",
       "11840               1  \n",
       "11841               1  \n",
       "11842               1  \n",
       "11843               1  \n",
       "11844               1  \n",
       "11845               1  \n",
       "11846               1  \n",
       "11847               1  \n",
       "11848               1  \n",
       "11849               1  \n",
       "11850               1  \n",
       "11851               1  \n",
       "11852               1  \n",
       "11853               1  \n",
       "11854               1  \n",
       "\n",
       "[8544 rows x 3 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = dataset[[\"sentence\",0,\"splitset_label\"]]\n",
    "d[d[\"splitset_label\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use that to split the dataset into 3 sets: training, validation and testing below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>0</th>\n",
       "      <th>splitset_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
       "      <td>0.69444</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
       "      <td>0.83333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Singer\\/composer Bryan Adams contributes a sle...</td>\n",
       "      <td>0.62500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>You 'd think by now America would have had eno...</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>Yet the act is still charming here .</td>\n",
       "      <td>0.72222</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sentence        0  splitset_label\n",
       "0   The Rock is destined to be the 21st Century 's...  0.69444               1\n",
       "1   The gorgeously elaborate continuation of `` Th...  0.83333               1\n",
       "60  Singer\\/composer Bryan Adams contributes a sle...  0.62500               1\n",
       "61  You 'd think by now America would have had eno...  0.50000               1\n",
       "62               Yet the act is still charming here .  0.72222               1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df_train = d[d[\"splitset_label\"] == 1]\n",
    "df_test = d[d[\"splitset_label\"] == 2]\n",
    "df_val = d[d[\"splitset_label\"] == 3]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more metric that we want to add to our dataset. TF-IDF is by far the most widely used measure of the *importance* of each word. TF stands for **Term Frequency**, which is exactly what it sounds like, increasing each time the word appears within each *document*. IDF stands for **Inversed Document Frequency**. This is a metric that decreases as a word occurs more often within a *corpus*.\n",
    "\n",
    "TF-IDF weights words as important if they are uniquely prevalent within the *document* in question. The math is below for those who are interested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### TF-IDF\n",
    "\n",
    "*term frequency* or *TF*: \n",
    "$$TF(w, d) = \\frac{count(w, d)}{\\sum_{v \\in V}count(v, d)}$$\n",
    "where $w, v$ are tokens (words), $V$ - vocabulary, $d$ - document in corpus\n",
    "\n",
    "*inversed document frequency* or *IDF*:\n",
    "$$IDF(w) = log \\frac{|D|}{\\sum_{d \\in D}\\mathbb{1}(w, d)} $$\n",
    "where $D$ is a corpus, $\\mathbb{1}$ is an indicator function of presence of specific token in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're going to transition from *Pandas* to *Keras*. While Pandas is used across data science to manage data, Keras is more specific to deep learning. \n",
    "\n",
    "From the Keras website: \n",
    "<pre>Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation.</pre>\n",
    "\n",
    "The ```Tokenizer``` class from Keras below implements the TF-IDF method of text analysis on our provided corpus. It will create the matrix representation of our text expected by neural network. One dimension of the matrix will be the number of the *token* and the other will be the TF-IDF weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the Tokenizer...\n",
      "Vectorizing sequence data...\n",
      "x_train shape: (8544, 20)\n",
      "x_test shape: (2210, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"Preparing the Tokenizer...\")\n",
    "max_words = 20 # We need a consisitent shape for our input data the vast majority of movie reviews are less than 20 words\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df_train[\"sentence\"])\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "x_train = tokenizer.texts_to_matrix(df_train[\"sentence\"], mode='binary')\n",
    "x_test = tokenizer.texts_to_matrix(df_test[\"sentence\"], mode='binary')\n",
    "x_val = tokenizer.texts_to_matrix(df_val[\"sentence\"], mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now created the inputs (or X values) that we will feed to our network. Let's take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  1., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  1.,  0., ...,  0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  1.,  0.,  0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we went from CSVs of movie reviews to a matrix of data. Neural networks don't understand words, or images, or driving, or sentiment. This makes our first job in in training neural networks converting the thing we care about into matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a matrix for our labels. One dimension again will be the number of the text, but the other one is little bit tricky: we need to produce one-hot encoding for the labels. One-hot encoding will be a zero vector by the length of the number of classes with a one at the position which corresponds to the actual label. Our objective is to classify each sentence as positive or negative, our labels will be of length 2, where positive sentiment is represented as [1,0] and negative sentiment is represented as [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convert class vector to binary class matrix (for use with categorical_crossentropy)\n",
      "y_train shape: (8544, 2)\n",
      "y_val shape: (1101, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(df_train[0], num_classes)\n",
    "y_val = keras.utils.to_categorical(df_val[0], num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_val shape:', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train shape: (8544, 2)\n",
      "y_val shape: (1101, 2)\n",
      "y_train shape: (8544, 2)\n",
      "negative:  3831 Positive:  4713\n",
      "negative:  510 Positive:  591\n",
      "negative:  3831 Positive:  4713\n"
     ]
    }
   ],
   "source": [
    "df_train2=[x+0.5 for x in df_train[0]]\n",
    "df_val2=[x+0.5 for x in df_val[0]]\n",
    "df_test2=[x+0.5 for x in df_train[0]]\n",
    "\n",
    "y_train = keras.utils.to_categorical(df_train2, num_classes)\n",
    "y_val = keras.utils.to_categorical(df_val2, num_classes)\n",
    "y_test = keras.utils.to_categorical(df_test2, num_classes)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "\n",
    "def countClasses2(testSet):\n",
    "    res= np.argmax(testSet, axis=1)\n",
    "    zeros=[i for i in res if i==0]\n",
    "    ones =[i for i in res if i==1]\n",
    "    print(\"negative: \", len(zeros), \"Positive: \", len(ones))\n",
    "    \n",
    "countClasses2(y_train)\n",
    "countClasses2(y_val)\n",
    "countClasses2(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       ..., \n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.],\n",
       "       [ 1.,  0.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing our model\n",
    "\n",
    "### Beginning with the simplest model\n",
    "\n",
    "#### Layer Types\n",
    "\n",
    "With ready data, our next job is to design our neural network. We will start with a simple architecture before we work towards identifying the ideal one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create our model in Keras. This model will consist of two ```Dense``` layers and some non-linear function, called an ```Activation```. \n",
    "\n",
    "```Dense``` layer is just matrix multiplication, used for general reasoning. We multiply our input matrix by a matrix of \"weights\" that change as our network trains to best map our inputs with our outputs.\n",
    "\n",
    "```Activation``` in this case is the Rectified Linear Unit, or **ReLU**. Nonlinearity removes the constraint that the relationship between our inputs and outputs are linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU is a common nonlinearity, and is defined by a simple formula:\n",
    "$$ReLU(z) = max(0, z)$$\n",
    "Here is its graphical representation (and next to another activation, sigmoid, for comparison):\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*XxxiA0jJvPrHEJHD4z893g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll use the SoftMax function. The softmax function is used in most *classification* models because its output is the probability that an input belongs to each class. Again, the math is below.\n",
    "\n",
    "\n",
    "$$SoftMax(x_i)=\\frac{e^{x_i}}{\\sum_{j=1..N}e^{x_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model in Keras\n",
    "\n",
    "The block below shows how we define the model we just described. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building simple model\n"
     ]
    }
   ],
   "source": [
    "print('Building simple model')\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just specified the architecture of our neural network. There are a few ways to visualize the network we've designed.\n",
    "\n",
    "First we'll use *yaml*, which stands for \"YAML ain't markup language.\" Seriously. That's what it stands for. YAML will let us read what we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend: tensorflow\n",
      "class_name: Sequential\n",
      "config:\n",
      "- class_name: Dense\n",
      "  config:\n",
      "    activation: linear\n",
      "    activity_regularizer: null\n",
      "    batch_input_shape: !!python/tuple [null, 20]\n",
      "    bias_constraint: null\n",
      "    bias_initializer:\n",
      "      class_name: Zeros\n",
      "      config: {}\n",
      "    bias_regularizer: null\n",
      "    dtype: float32\n",
      "    kernel_constraint: null\n",
      "    kernel_initializer:\n",
      "      class_name: VarianceScaling\n",
      "      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "    kernel_regularizer: null\n",
      "    name: dense_1\n",
      "    trainable: true\n",
      "    units: 16\n",
      "    use_bias: true\n",
      "- class_name: Activation\n",
      "  config: {activation: relu, name: activation_1, trainable: true}\n",
      "- class_name: Dense\n",
      "  config:\n",
      "    activation: linear\n",
      "    activity_regularizer: null\n",
      "    bias_constraint: null\n",
      "    bias_initializer:\n",
      "      class_name: Zeros\n",
      "      config: {}\n",
      "    bias_regularizer: null\n",
      "    kernel_constraint: null\n",
      "    kernel_initializer:\n",
      "      class_name: VarianceScaling\n",
      "      config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "    kernel_regularizer: null\n",
      "    name: dense_2\n",
      "    trainable: true\n",
      "    units: 2\n",
      "    use_bias: true\n",
      "- class_name: Activation\n",
      "  config: {activation: softmax, name: activation_2, trainable: true}\n",
      "keras_version: 2.0.9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "\n",
    "yaml_string = model.to_yaml()\n",
    "model = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to draw our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"387pt\" viewBox=\"0.00 0.00 302.00 387.00\" width=\"302pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 383)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-383 298,-383 298,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 139784688727320 -->\n",
       "<g class=\"node\" id=\"node1\"><title>139784688727320</title>\n",
       "<polygon fill=\"none\" points=\"0,-332.5 0,-378.5 294,-378.5 294,-332.5 0,-332.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-351.8\">dense_1_input: InputLayer</text>\n",
       "<polyline fill=\"none\" points=\"163,-332.5 163,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190.5\" y=\"-363.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"163,-355.5 218,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"190.5\" y=\"-340.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"218,-332.5 218,-378.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256\" y=\"-363.3\">(None, 20)</text>\n",
       "<polyline fill=\"none\" points=\"218,-355.5 294,-355.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"256\" y=\"-340.3\">(None, 20)</text>\n",
       "</g>\n",
       "<!-- 139784688726480 -->\n",
       "<g class=\"node\" id=\"node2\"><title>139784688726480</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-249.5 30.5,-295.5 263.5,-295.5 263.5,-249.5 30.5,-249.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-268.8\">dense_1: Dense</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-249.5 132.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-280.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-272.5 187.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-257.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-249.5 187.5,-295.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-280.3\">(None, 20)</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-272.5 263.5,-272.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-257.3\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 139784688727320&#45;&gt;139784688726480 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>139784688727320-&gt;139784688726480</title>\n",
       "<path d=\"M147,-332.366C147,-324.152 147,-314.658 147,-305.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"150.5,-305.607 147,-295.607 143.5,-305.607 150.5,-305.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139784688726816 -->\n",
       "<g class=\"node\" id=\"node3\"><title>139784688726816</title>\n",
       "<polygon fill=\"none\" points=\"7.5,-166.5 7.5,-212.5 286.5,-212.5 286.5,-166.5 7.5,-166.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-185.8\">activation_1: Activation</text>\n",
       "<polyline fill=\"none\" points=\"155.5,-166.5 155.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-197.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"155.5,-189.5 210.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-174.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"210.5,-166.5 210.5,-212.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.5\" y=\"-197.3\">(None, 16)</text>\n",
       "<polyline fill=\"none\" points=\"210.5,-189.5 286.5,-189.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.5\" y=\"-174.3\">(None, 16)</text>\n",
       "</g>\n",
       "<!-- 139784688726480&#45;&gt;139784688726816 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>139784688726480-&gt;139784688726816</title>\n",
       "<path d=\"M147,-249.366C147,-241.152 147,-231.658 147,-222.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"150.5,-222.607 147,-212.607 143.5,-222.607 150.5,-222.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139784688726368 -->\n",
       "<g class=\"node\" id=\"node4\"><title>139784688726368</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-83.5 30.5,-129.5 263.5,-129.5 263.5,-83.5 30.5,-83.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-102.8\">dense_2: Dense</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-83.5 132.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-114.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"132.5,-106.5 187.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"160\" y=\"-91.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-83.5 187.5,-129.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-114.3\">(None, 16)</text>\n",
       "<polyline fill=\"none\" points=\"187.5,-106.5 263.5,-106.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"225.5\" y=\"-91.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 139784688726816&#45;&gt;139784688726368 -->\n",
       "<g class=\"edge\" id=\"edge3\"><title>139784688726816-&gt;139784688726368</title>\n",
       "<path d=\"M147,-166.366C147,-158.152 147,-148.658 147,-139.725\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"150.5,-139.607 147,-129.607 143.5,-139.607 150.5,-139.607\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 139784688597368 -->\n",
       "<g class=\"node\" id=\"node5\"><title>139784688597368</title>\n",
       "<polygon fill=\"none\" points=\"11,-0.5 11,-46.5 283,-46.5 283,-0.5 11,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"85\" y=\"-19.8\">activation_2: Activation</text>\n",
       "<polyline fill=\"none\" points=\"159,-0.5 159,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186.5\" y=\"-31.3\">input:</text>\n",
       "<polyline fill=\"none\" points=\"159,-23.5 214,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"186.5\" y=\"-8.3\">output:</text>\n",
       "<polyline fill=\"none\" points=\"214,-0.5 214,-46.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.5\" y=\"-31.3\">(None, 2)</text>\n",
       "<polyline fill=\"none\" points=\"214,-23.5 283,-23.5 \" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"248.5\" y=\"-8.3\">(None, 2)</text>\n",
       "</g>\n",
       "<!-- 139784688726368&#45;&gt;139784688597368 -->\n",
       "<g class=\"edge\" id=\"edge4\"><title>139784688726368-&gt;139784688597368</title>\n",
       "<path d=\"M147,-83.3664C147,-75.1516 147,-65.6579 147,-56.7252\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"150.5,-56.6068 147,-46.6068 143.5,-56.6069 150.5,-56.6068\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final touch: declaring the loss function for the model. At a high-level, loss is the function that is minimized as our model trains. As loss decreases, we approach the ideal solution for our function. In this case, we've chosen the \"categorical_crossentropy\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "We've taken a corpus of movie reviews which have been labeled by sentiment, converted both the sentences and labels into data our network understands, and designed a simple neural network. We're finally ready to train the model.\n",
    "\n",
    "Start by defining a few hyperparameters: how fast and how long to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking Keras to train your model is done with the call <pre> model.fit </pre> See if you can identify each of the parameters in this call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7689 samples, validate on 855 samples\n",
      "Epoch 1/5\n",
      "7689/7689 [==============================] - 5s 641us/step - loss: 0.3018 - acc: 0.8857 - val_loss: 0.1263 - val_acc: 1.0000\n",
      "Epoch 2/5\n",
      "7689/7689 [==============================] - 1s 106us/step - loss: 0.0463 - acc: 0.9973 - val_loss: 0.0566 - val_acc: 1.0000\n",
      "Epoch 3/5\n",
      "7689/7689 [==============================] - 1s 110us/step - loss: 0.0300 - acc: 0.9973 - val_loss: 0.0352 - val_acc: 1.0000\n",
      "Epoch 4/5\n",
      "7689/7689 [==============================] - 1s 108us/step - loss: 0.0263 - acc: 0.9973 - val_loss: 0.0253 - val_acc: 1.0000\n",
      "Epoch 5/5\n",
      "7689/7689 [==============================] - 1s 159us/step - loss: 0.0248 - acc: 0.9973 - val_loss: 0.0199 - val_acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard  \n",
    "tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210/2210 [==============================] - 0s 34us/step\n"
     ]
    }
   ],
   "source": [
    "results = model.predict(x_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As expected, loss decreased during training. Our model was able to fit to the data over time. When in doubt, starting with a simple model and adding complexity is recommended. However, there are some workflows where we have more insight about where to start. Let's see if we can do better with recurrent neural networks, which are now the industry standard for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models designed for natural language processing: RNNs\n",
    "\n",
    "The simple network we trained in the last section was intentionally the simplest network we could have built. Without any insight as to where to start, starting simple is usually recommended. However, there are networks that are known to be particularly well matched for specific types of problems. \n",
    "\n",
    "An examination of [DL4J's great problem type to network type summary](https://deeplearning4j.org/neuralnetworktable) shows that for all text, sound, and time-series tasks are well matched for some form of Recurrent Networks (RNN, RNTN, etc.). The need for memory is the main characteristic that sets these networks apart.  **The nature of language is that each word is understood through the context of the words that came before it.**\n",
    "\n",
    "In Recurrent Neural Networks (RNNs), the output of the network after each prediction becomes a part of the input of the next prediction. \n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/759/1*UkI9za9zTR-HL8uM15Wmzw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for an RNN\n",
    "\n",
    "Planning to use the right type of network for the problem means that we're more likely to find datasets designed for our model. In this case, keras has a dataset called [IMDB](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) which is tailor made to train sentiment classification models.\n",
    "\n",
    "We can load the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
      "17465344/17464789 [==============================] - 0s 0us/step\n",
      "25000 train sequences\n",
      "25000 test sequences\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import imdb\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pad (or trim) sentences to a maximum length, maxlen, so our RNN can work with them in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 80)\n",
      "x_test shape: (25000, 80)\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import sequence\n",
    "print('Pad sequences (samples x time)')\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer types for recurrent networks\n",
    "\n",
    "The ideal type of layer for this type of network is called a Long-Short Term Memory (LSTM) layer. LSTMs hold memory until something in the data indicates that it should be forgotten. For example, a model may learn that a period or end of paragraph is a signal to forget some aspects of the model. The math of an LSTM is below for those who are interested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "Formulae:\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1)\n",
    "\n",
    "\n",
    "And on figure:\n",
    "![](https://www.researchgate.net/profile/Marijn_Stollenga/publication/304346489/figure/fig13/AS:376211038588933@1466707109201/Figure-74-RNN-and-LSTM-A-graphical-representation-of-the-RNN-and-LSTM-networks-are.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an RNN\n",
    "\n",
    "To build an RNN, we don't need to know everything about the math of an LSTM. To build an RNN, we tell Keras to use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we introduce two new layers: ```Embedding``` - the layer which learns a vector for each word, and ```LSTM``` - which is just an LSTM cell described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following training cell for the number of epochs we have specified. This will take about 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/5\n",
      "22500/22500 [==============================] - 101s 5ms/step - loss: 0.6814 - acc: 0.5608 - val_loss: 0.6713 - val_acc: 0.5960\n",
      "Epoch 2/5\n",
      "22500/22500 [==============================] - 100s 4ms/step - loss: 0.6705 - acc: 0.5844 - val_loss: 0.6777 - val_acc: 0.5816\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 26s 1ms/step\n",
      "Test score: 0.67773174221\n",
      "Test accuracy: 0.5764\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "The score is better, but not much. You can improve it dramatically by adding some layers or tweaking some hyperparameters. Be creative! Your goal is to reach 0.75 on this dataset, but it is not the maximum achievable limit, just a target with the time you have to complete this lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we can do text classification then many identification tasks open up to apply the same approach on. One example is [MBTI][https://www.kaggle.com/datasnaek/mbti-type) where people's personalities can be divided into 16 different types. The dataset includes writing samples from each of the personality types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Take home exercise to use the kaggle dataset to see if people's personality can be discerned based on the online written samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's not possible to do the analysis of the whole document in one shot so it has to be broken down into small steps. The strategy we will employ is called *part of speech tagging* or simply *PoS-tagging*; which is a markup of a sentence by PoS for every word. These tags for every word can then be used for tasks such as *text classification* which we will do in this lab.\n",
    "\n",
    "Now, let us import all the libraries to setup our text classification process. We will be utilizing Keras framework for convenience and utilities like numpy for ease of use. Run the following cell by clicking inside of it and pressing (**Shift + Enter**). This will bring in the proper libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start our Natural Language Processing (NLP) journey with classification, because while it is a basic step in understanding natural languages, it is still a very practical start. Once we can figure out the meaning of a word then more complex tasks are possible such as sentiment analysis. Knowing sentiment of the words are very useful and common for many industries. For example, online reviews or comments are a common way for any big company to track their public image and how customers feel about them. Or used by companies like [Bloomberg](https://www.cio.com.au/article/628705/bloomberg-big-move-machine-learning-open-source) to process live news or tweets for a fuller picture to make better investment decisions. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
