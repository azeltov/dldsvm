{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with Keras\n",
    "Author: [Valentin Malykh](http://val.maly.hk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we are going to introduce Natural Language Processing (NLP) through the task of text classification. Here, we train a neural network to identify *types* of text samples: positive samples from negative samples, samples from different writers, etc.\n",
    "\n",
    "Like many challenges in deep learning, we're working to train a neural network to map between provided inputs and desired outputs. Let's start by understanding how to prepare text for input to a neural network. \n",
    "\n",
    "## Input data\n",
    "\n",
    "First, let us define some terms used in Natural Language Processing (NLP) to describe language as input data:\n",
    "\n",
    "- *token* - A unit of text, it could be a word (and almost always is), but also it could be a group of words like \"New York\", a sub-word like \"mega\" in \"megabyte\", or a letter like \"m\". Each token can be represented as a distinct number in a process called *tokenizing*.  \n",
    "- *document* - A sequence of *tokens,* this could be whole book or a tweet. In this case, we're going to classify each *document* as having a positive or a negative sentiment.  \n",
    "- *corpus* - A set of *documents.* You can think of this as your \"dataset\". If you want to learn about language in general, you might use a *corpus* like Wikipedia. In this case, we'll use a *corpus* containing movie reviews since each *document* is paired with a rating indicating *sentiment.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Steps\n",
    "\n",
    "In this lab, we are going to perform *text classification* for *sentiment analysis*. We will classify each document into one of 2 different sentiments: \"positive\" or \"negative\". \n",
    "\n",
    "Our training *corpus* will be the Sentiment Tree Bank from [Stanford's NLP group](https://nlp.stanford.edu/sentiment/). The dataset's creators at Stanford describe the benefit of using Deep Learning for language processing vs. using traditional methods below:\n",
    "\n",
    "```\"Most sentiment prediction systems work just by looking at words in isolation, giving positive points for positive words and negative points for negative words and then summing up these points. That way, the order of words is ignored, and important information is lost. In contrast, our new deep learning model builds a representation of whole sentences based on the sentence structure. It computes the sentiment based on how words compose the meaning of longer phrases. This way, the model is not as easily fooled as previous models. For example, our model learned that funny and witty are positive, but the following sentence is still negative overall:```\n",
    "\n",
    "*This movie was actually neither that funny, nor super witty.*\"\n",
    "\n",
    "The idea is to understand the overall context of each word used along with its meaning to figure out the emotional tone of the document. Look at the [dataset](https://nlp.stanford.edu/sentiment/treebank.html) before loading libraries, downloading the dataset, and unzipping it into our workspace below.\n",
    "\n",
    "This Jupyter Notebook runs python code in code blocks like the one below. Click inside the cell and simultaneously press **Shift + Enter** to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Input\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import pandas\n",
    "! if [ ! -f stanfordSentimentTreebank.zip ]; then wget http://nlp.stanford.edu/~socherr/stanfordSentimentTreebank.zip; fi\n",
    "! unzip stanfordSentimentTreebank.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view and manage the data, we'll use a tool called **Pandas**. Pandas is an open source library providing high-performance, easy-to-use data structures and data analysis tools. Our corpus is currently stored in a CSV. We'll use Pandas here to read and view CSV data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = pandas.read_csv(\"stanfordSentimentTreebank/datasetSentences.txt\", sep=\"\\t\")\n",
    "sentences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see (if you scroll to the bottom) that our corpus contained 11855 sentences each with a unique identifier.\n",
    "\n",
    "Next, we need each sentence's corresponding label to identify if the sentence contained a positive or negative sentiment. \n",
    "\n",
    "When we do a *read* with pandas, it creates a base object called DataFrame. DataFrame is represented as a *numpy array*, an ideal format for feeding to a neural network.\n",
    "\n",
    "Below, we will use another property of DataFrame, column access. ```sentences[\"sentence\"]``` will return only one specific column of this particular DataFrame, the one labeled \"sentence\". ```tolist()``` returns a python list instead of a Series (another base class in Pandas). Execute the following cells to prepare the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sent_labels(sentences):\n",
    "    dictionary = dict()\n",
    "    with open(\"stanfordSentimentTreebank/dictionary.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            splitted = line.split(\"|\")\n",
    "            dictionary[splitted[0].lower()] = int(splitted[1])\n",
    "\n",
    "    labels = [0.5] * (max(dictionary.values()) + 1)\n",
    "    with open(\"stanfordSentimentTreebank/sentiment_labels.txt\", \"rt\", encoding=\"utf-8\") as f:\n",
    "        f.readline()\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            splitted = line.split(\"|\")\n",
    "            labels[int(splitted[0])] = float(splitted[1])\n",
    "\n",
    "    sent_labels = [0.5] * len(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        full_sent = sentences[i].replace(\"-lrb-\", \"(\").replace(\"-rrb-\", \")\").replace(\"\\\\\\\\\", \"\")\n",
    "        try:\n",
    "            sent_labels[i] = labels[dictionary[full_sent.lower()]]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    return sent_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create labels and check how many sentences there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = sent_labels(sentences=sentences[\"sentence\"].tolist())\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a large enough dataset for us to start working with. \n",
    "\n",
    "Lastly, like any deep learning workflow, we should separate our dataset into a training set (that our network will learn from), a validation set (that our network will *NOT* learn from, so that we can see if our model is effective with *new* data) and a test set (that our network will *NOT* learn from, used to visualize what *types* of sentences confuse our model.)\n",
    "\n",
    "The creators at Stanford have helped us do this by including a \"split\" csv. Let's load that with Pandas now: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = pandas.read_csv(\"stanfordSentimentTreebank/datasetSplit.txt\")\n",
    "split.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here the same unique identifiers as with the sentences above along with a split label: 1, 2, or 3. The sentences and labels that correspond to 1 belong to our training dataset, 2 belong to our validation set, and 3 belong to our test set.\n",
    "\n",
    "Lastly, we can concatenate our sentences, labels, and split indicators to create our comprehensive dataset. Note that ```concat``` will concatenate DataFrames (and Series) even if they are of different lengths. This flexibility is another reason we are utilizing Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = pandas.concat([sentences, pandas.DataFrame(labels), split], axis=1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the following cell to see how you can use Pandas to filter. By changing \"splitset_label\" from 1 to 2 to 3, you will see the training set, validation set, and test set respectively. \n",
    "\n",
    "First select some columns by their names - ```dataset[[\"sentence\",0,\"splitset_label\"]]```, and next filter the produced DataFrame by value of one of its columns ```d[d[\"splitset_label\"] == 1]```.\n",
    "\n",
    "Also, if you call a DataFrame in Jupyter, it is an equivalent of ```head()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = dataset[[\"sentence\",0,\"splitset_label\"]]\n",
    "d[d[\"splitset_label\"] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use that to split the dataset into 3 sets: training, validation and testing below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "df_train = d[d[\"splitset_label\"] == 1]\n",
    "df_test = d[d[\"splitset_label\"] == 2]\n",
    "df_val = d[d[\"splitset_label\"] == 3]\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one more metric that we want to add to our dataset. TF-IDF is by far the most widely used measure of the *importance* of each word. TF stands for **Term Frequency**, which is exactly what it sounds like, increasing each time the word appears within each *document*. IDF stands for **Inversed Document Frequency**. This is a metric that decreases as a word occurs more often within a *corpus*.\n",
    "\n",
    "TF-IDF weights words as important if they are uniquely prevalent within the *document* in question. The math is below for those who are interested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### TF-IDF\n",
    "\n",
    "*term frequency* or *TF*: \n",
    "$$TF(w, d) = \\frac{count(w, d)}{\\sum_{v \\in V}count(v, d)}$$\n",
    "where $w, v$ are tokens (words), $V$ - vocabulary, $d$ - document in corpus\n",
    "\n",
    "*inversed document frequency* or *IDF*:\n",
    "$$IDF(w) = log \\frac{|D|}{\\sum_{d \\in D}\\mathbb{1}(w, d)} $$\n",
    "where $D$ is a corpus, $\\mathbb{1}$ is an indicator function of presence of specific token in a document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we're going to transition from *Pandas* to *Keras*. While Pandas is used across data science to manage data, Keras is more specific to deep learning. \n",
    "\n",
    "From the Keras website: \n",
    "<pre>Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation.</pre>\n",
    "\n",
    "The ```Tokenizer``` class from Keras below implements the TF-IDF method of text analysis on our provided corpus. It will create the matrix representation of our text expected by neural network. One dimension of the matrix will be the number of the *token* and the other will be the TF-IDF weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Preparing the Tokenizer...\")\n",
    "max_words = 20 # We need a consisitent shape for our input data the vast majority of movie reviews are less than 20 words\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df_train[\"sentence\"])\n",
    "\n",
    "print('Vectorizing sequence data...')\n",
    "x_train = tokenizer.texts_to_matrix(df_train[\"sentence\"], mode='binary')\n",
    "x_test = tokenizer.texts_to_matrix(df_test[\"sentence\"], mode='binary')\n",
    "x_val = tokenizer.texts_to_matrix(df_val[\"sentence\"], mode='binary')\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now created the inputs (or X values) that we will feed to our network. Let's take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that we went from CSVs of movie reviews to a matrix of data. Neural networks don't understand words, or images, or driving, or sentiment. This makes our first job in in training neural networks converting the thing we care about into matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to create a matrix for our labels. One dimension again will be the number of the text, but the other one is little bit tricky: we need to produce one-hot encoding for the labels. One-hot encoding will be a zero vector by the length of the number of classes with a one at the position which corresponds to the actual label. Our objective is to classify each sentence as positive or negative, our labels will be of length 2, where positive sentiment is represented as [1,0] and negative sentiment is represented as [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Convert class vector to binary class matrix '\n",
    "      '(for use with categorical_crossentropy)')\n",
    "num_classes = 2\n",
    "y_train = keras.utils.to_categorical(df_train[0], num_classes)\n",
    "y_val = keras.utils.to_categorical(df_val[0], num_classes)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('y_val shape:', y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing our model\n",
    "\n",
    "### Beginning with the simplest model\n",
    "\n",
    "#### Layer Types\n",
    "\n",
    "With ready data, our next job is to design our neural network. We will start with a simple architecture before we work towards identifying the ideal one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create our model in Keras. This model will consist of two ```Dense``` layers and some non-linear function, called an ```Activation```. \n",
    "\n",
    "```Dense``` layer is just matrix multiplication, used for general reasoning. We multiply our input matrix by a matrix of \"weights\" that change as our network trains to best map our inputs with our outputs.\n",
    "\n",
    "```Activation``` in this case is the Rectified Linear Unit, or **ReLU**. Nonlinearity removes the constraint that the relationship between our inputs and outputs are linear."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU is a common nonlinearity, and is defined by a simple formula:\n",
    "$$ReLU(z) = max(0, z)$$\n",
    "Here is its graphical representation (and next to another activation, sigmoid, for comparison):\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*XxxiA0jJvPrHEJHD4z893g.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we'll use the SoftMax function. The softmax function is used in most *classification* models because its output is the probability that an input belongs to each class. Again, the math is below.\n",
    "\n",
    "\n",
    "$$SoftMax(x_i)=\\frac{e^{x_i}}{\\sum_{j=1..N}e^{x_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building the model in Keras\n",
    "\n",
    "The block below shows how we define the model we just described. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Building simple model')\n",
    "model = Sequential()\n",
    "model.add(Dense(16, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just specified the architecture of our neural network. There are a few ways to visualize the network we've designed.\n",
    "\n",
    "First we'll use *yaml*, which stands for \"YAML ain't markup language.\" Seriously. That's what it stands for. YAML will let us read what we've built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model.to_yaml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_yaml\n",
    "\n",
    "yaml_string = model.to_yaml()\n",
    "model = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to draw our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "plot_model(model, to_file='model.png', show_shapes=True)\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final touch: declaring the loss function for the model. At a high-level, loss is the function that is minimized as our model trains. As loss decreases, we approach the ideal solution for our function. In this case, we've chosen the \"categorical_crossentropy\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model\n",
    "\n",
    "We've taken a corpus of movie reviews which have been labeled by sentiment, converted both the sentences and labels into data our network understands, and designed a simple neural network. We're finally ready to train the model.\n",
    "\n",
    "Start by defining a few hyperparameters: how fast and how long to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking Keras to train your model is done with the call <pre> model.fit </pre> See if you can identify each of the parameters in this call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard  \n",
    "#tensorboard=TensorBoard(log_dir='./logs', write_graph=True, write_images=True)\n",
    "from keras.callbacks import EarlyStopping  \n",
    "early_stopping=EarlyStopping(monitor='val_loss')  \n",
    "\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = model.predict(x_test, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "As expected, loss decreased during training. Our model was able to fit to the data over time. When in doubt, starting with a simple model and adding complexity is recommended. However, there are some workflows where we have more insight about where to start. Let's see if we can do better with recurrent neural networks, which are now the industry standard for NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models designed for natural language processing: RNNs\n",
    "\n",
    "The simple network we trained in the last section was intentionally the simplest network we could have built. Without any insight as to where to start, starting simple is usually recommended. However, there are networks that are known to be particularly well matched for specific types of problems. \n",
    "\n",
    "An examination of [DL4J's great problem type to network type summary](https://deeplearning4j.org/neuralnetworktable) shows that for all text, sound, and time-series tasks are well matched for some form of Recurrent Networks (RNN, RNTN, etc.). The need for memory is the main characteristic that sets these networks apart.  **The nature of language is that each word is understood through the context of the words that came before it.**\n",
    "\n",
    "In Recurrent Neural Networks (RNNs), the output of the network after each prediction becomes a part of the input of the next prediction. \n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/759/1*UkI9za9zTR-HL8uM15Wmzw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for an RNN\n",
    "\n",
    "Planning to use the right type of network for the problem means that we're more likely to find datasets designed for our model. In this case, keras has a dataset called [IMDB](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification) which is tailor made to train sentiment classification models.\n",
    "\n",
    "We can load the data below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_words)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pad (or trim) sentences to a maximum length, maxlen, so our RNN can work with them in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "print('Pad sequences (samples x time)')\n",
    "maxlen = 80  # cut texts after this number of words (among top max_features most common words)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer types for recurrent networks\n",
    "\n",
    "The ideal type of layer for this type of network is called a Long-Short Term Memory (LSTM) layer. LSTMs hold memory until something in the data indicates that it should be forgotten. For example, a model may learn that a period or end of paragraph is a signal to forget some aspects of the model. The math of an LSTM is below for those who are interested. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "Formulae:\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/2db2cba6a0d878e13932fa27ce6f3fb71ad99cf1)\n",
    "\n",
    "\n",
    "And on figure:\n",
    "![](https://www.researchgate.net/profile/Marijn_Stollenga/publication/304346489/figure/fig13/AS:376211038588933@1466707109201/Figure-74-RNN-and-LSTM-A-graphical-representation-of-the-RNN-and-LSTM-networks-are.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an RNN\n",
    "\n",
    "To build an RNN, we don't need to know everything about the math of an LSTM. To build an RNN, we tell Keras to use them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we introduce two new layers: ```Embedding``` - the layer which learns a vector for each word, and ```LSTM``` - which is just an LSTM cell described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following training cell for the number of epochs we have specified. This will take about 10 minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(x_test, y_test,\n",
    "                            batch_size=batch_size)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 1\n",
    "\n",
    "The score is better, but not much. You can improve it dramatically by adding some layers or tweaking some hyperparameters. Be creative! Your goal is to reach 0.75 on this dataset, but it is not the maximum achievable limit, just a target with the time you have to complete this lab. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we can do text classification then many identification tasks open up to apply the same approach on. One example is [MBTI][https://www.kaggle.com/datasnaek/mbti-type) where people's personalities can be divided into 16 different types. The dataset includes writing samples from each of the personality types. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "Take home exercise to use the kaggle dataset to see if people's personality can be discerned based on the online written samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But it's not possible to do the analysis of the whole document in one shot so it has to be broken down into small steps. The strategy we will employ is called *part of speech tagging* or simply *PoS-tagging*; which is a markup of a sentence by PoS for every word. These tags for every word can then be used for tasks such as *text classification* which we will do in this lab.\n",
    "\n",
    "Now, let us import all the libraries to setup our text classification process. We will be utilizing Keras framework for convenience and utilities like numpy for ease of use. Run the following cell by clicking inside of it and pressing (**Shift + Enter**). This will bring in the proper libraries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start our Natural Language Processing (NLP) journey with classification, because while it is a basic step in understanding natural languages, it is still a very practical start. Once we can figure out the meaning of a word then more complex tasks are possible such as sentiment analysis. Knowing sentiment of the words are very useful and common for many industries. For example, online reviews or comments are a common way for any big company to track their public image and how customers feel about them. Or used by companies like [Bloomberg](https://www.cio.com.au/article/628705/bloomberg-big-move-machine-learning-open-source) to process live news or tweets for a fuller picture to make better investment decisions. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
